# 1 爬取文件
>依赖的库：`requests`，用于发送请求
## 1.1 HTTP请求方法
- GET方法：获得数据
- POST方法：创建数据（如登录网站）
## 1.2 HTTP请求结构
- 这些信息会自动生成，除非需要修改无需手动输入
- POST方法
```Python
#请求行：包含方法类型（POST）、资源路径（/user/info）、查询参数（?后的内容）、协议版本（HTTP/1.1）
POST /user/info?查询参数 HTTP/1.1
#请求头：包含给服务器的信息
#第一行：主机域名，结合请求行里的资源路径组成一个完整的网址
#第二行：告诉服务器客户端的相关信息
#第三行：告诉服务器客户端想接受的响应数据的类型
Host:www.example.com 
User-Agent:curl/7.77.0
Accept:*/*
#请求体：发送给服务端的数据
{"username":"your_name"
 "email":"email_name@mail_name.com"}
```
- GET方法的请求体一般是空的
```Python
#请求行
POST /user/info?查询参数 HTTP/1.1
#请求头
Host: www.example.com 
User-Agent: curl/7.77.0
Accept: */*
#请求体为空
```
## 1.3 HTTP响应结构
- 这是服务器返回给我们的东西，是自动生成的
```Python
#状态行：包含协议版本（HTTP/1.1）、状态码（200）、状态消息（OK），其中状态码和状态消息是对应的，如`200 OK`表示客户端请求成功，`404 Not Found`表示请求资源不存在
HTTP/1.1 200 OK
#响应头：一些信息
#第一行：生成响应的日期时间
#第二行：返回内容的类型（html）及编码格式（utf-8）
Date:Fri, 27 Jan 2024 02:10:48 GMT
Content-Type:text/html;charset=utf-8
#响应体：对应的内容类型的文件，如这里是html文件
<!DOCTYPE html>
	<><>
	<><>
</html>
```
## 1.4 伪装成浏览器
- 有的时候网站会拒绝爬虫程序访问，此时可以通过定义请求头把程序伪装成浏览器
- 通过进入网页，按F12，进入`Network`板块，找到`User-Agent`板块，把冒号后的内容放到这即可
![](获取请求头.png)
- 接着可以获取到网页相关信息了
```Python
import requests

#篡改请求头内容
headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0"}

#输入想爬取的完整url，其中改变了headers这个可选参数的值，作为伪装
response = requests.get("https://www.bilibili.com/video/BV1sc411v7m2/?spm_id_from=333.999.0.0&vd_source=6fbd8ea5d181239758f62d5f9a9d8dfb", headers=headers)

#此时可以查看响应结果状态码，成功则是200
print(response.status_code)

#如果响应成功则xxx，反之则打印失败信息
if response.ok:
    #拿到页面的html源代码
    print(response.text)
else:
    print("Requests Fail")
```
# 2 文本解析
>依赖的库：`bs4`，包含用来做HTML解析的`Beautiful Soup`，也可以直接分析网页JSON文件
## 2.1 爬取b站up主视频相关信息
```Python
from bs4 import BeautifulSoup
import requests

#篡改请求头内容
headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0"}

#输入想爬取的完整url，其中改变了headers这个可选参数的值，作为伪装
#这里爬取一个up主的视频信息
response = requests.get("https://www.bilibili.com/video/BV1sc411v7m2/?spm_id_from=333.999.0.0&vd_source=6fbd8ea5d181239758f62d5f9a9d8dfb", headers=headers)

#此时可以查看响应结果状态码，成功则是200
print(response.status_code)

#拿到页面的html源代码
#print(response.text)

#指定`html.parser`为内容解析器
soup = BeautifulSoup(response.text, "html.parser")

#这样会输出所得的html文件中的第一个含p元素及其连带的所有内容
#print(soup.p)

#findAll会返回一个可遍历对象；根据属性（class），找出所有属性为title的元素
allP = soup.findAll("p", attrs={"class" : "title"})
#可以不传入attrs参数，这里即找出所有p标签内容
#allP = soup.findAll("p")

#打印页面中所有储存了title信息的p标签
for i in allP:
    #若是print(i)则打印html中所有p标签的内容；i.string表示只打印对象的字符串属性内容，把被标签包围的文字信息返回给我们
    print(i.string)

    #继续在这些p标签元素内查找a标签内容，查找class属性为title的内容
    #allTitle = i.findAll("a", attrs={"class" : "title"})
    #for j in allTitle:
        #print(j.string)

#'w'是以写入模式（write mode），即打开文件的一种方式,以此模式打开文件时，若文件不存在，则会创建新文件；若已存在，则会覆盖已有内容
#这个语句会在退出with块时自动关闭文件，无需手动调用close()方法
with open("info.txt", "w", encoding="utf-8") as file:
    for i in allP:
        file.write(i.string.strip() + "\n")
```
## 2.2 其他案例展示
### 2.2.1 爬取b站番剧排行
- 此处运用JSON文件分析，其余的如发送HTTP请求与上面一致
```Python
import requests

#模拟浏览器进行访问
header = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0"}

#b站榜单接口链接
url = 'https://api.bilibili.com/pgc/web/rank/list?day=3&season_type=1'

#获取接口返回的数据内容
res = requests.get(url, headers=header)

#将内容解析为JSON格式，存储在data变量中
data = res.json()

#根据返回的data字典结构，获取视频列表字段
#"result"和"list"是由哔哩哔哩网站的API接口返回的JSON数据结构所决定的
video_list = data["result"]["list"]

#提取需要的视频关键信息，并写入txt文件
#'w'是以写入模式（write mode），即打开文件的一种方式,以此模式打开文件时，若文件不存在，则会创建新文件；若已存在，则会覆盖已有内容
#这个语句会在退出with块时自动关闭文件，无需手动调用close()方法
with open('./'+'video.txt','w',encoding='utf-8') as f:
    for item in video_list:
        text = str(item['rank']) + item['title'] + item['rating'] + item['url']
        print('写入数据 <'+text+'> 成功')
        f.write('\n'+text)
```
- 爬取结果前两行展示如下
```
1葬送的芙莉莲9.9分https://www.bilibili.com/bangumi/play/ss46089
2迷宫饭9.8分https://www.bilibili.com/bangumi/play/ss47083
```
### 2.2.2 爬取搜狗百科新闻
```Python
import json
import requests
from bs4 import BeautifulSoup

UserAgent = 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0'

def crawl_data():
    """
    爬取搜狗百科内容
    """
    headers = {
        'User-Agent': UserAgent,
    }
    url='https://baike.sogou.com/'

    response = requests.get(url,headers=headers)

    # 将一段文档传入BeautifulSoup的构造方法,就能得到一个文档的对象, 可以传入一段字符串
    soup = BeautifulSoup(response.content,'html.parser')
    col_div = soup.find('div',class_ = 'column_content')
    news_list = col_div.find('ol')
    return news_list

def parse_news_data(ol_html):
    '''
    解析得到选手信息，包括包括选手姓名和选手个人百度百科页面链接，存JSON文件,保存到当前目录
    '''
    bs = BeautifulSoup(str(ol_html),'html.parser')
    all_lis = bs.find_all('li')
    news_list = []
    for li in all_lis:
        news = {}    
        #找选手名称和选手百度百科连接
        new_a = li.find('a')
        if new_a:
            news["title"]=new_a.text
            news['link'] =  'https://baike.sogou.com/' + new_a.get('href')
            news_list.append(news)
    json_data = json.loads(str(news_list).replace("\'","\""))  
    # 将文件存储到work目录下
    with open('./' + 'news.json', 'w', encoding='UTF-8') as f:
        json.dump(json_data, f, ensure_ascii=False)

aa = crawl_data()
parse_news_data(aa)
```
- 爬取结果部分如下
```
[{"title": "朴敏英在新剧采访中自曝患有失温、抑郁症", "link": "https://baike.sogou.com//v1854564.htm"}
```