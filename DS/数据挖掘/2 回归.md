# 2.1 回归任务简介
- 回归（Regression） 任务是监督学习中的一大类任务，和分类任务不同，其目标是预测一个连续变量的值
# 2.2 回归模型
## 2.2.1 模型的选取
- 关于如何选取模型来拟合回归曲线，可能的思路：从简单到复杂尝试多种模型，直到满足性能要求；使用多个简单的模型进行集成学习等
- 常见的模型有线性回归拟合（Linear Regression）与非线性回归拟合（Non Linear Regression）
![](regress1.png)
![](regress2.png)
## 2.2.2 集成学习（Ensemble Learning）
- 弱可学习：存在一个多项式算法可以学习它并且准确率仅略比随机猜测略好
- 强可学习：存在一个多项式算法可以学习它并且准确率很高
- 一个概念是强可学习的的充要条件是它是弱可学习的
- 集成学习：设计一组弱学习器，集成起来构成一个强学习器
![](regress3.png)
- 模型并非越多越复杂越好，这样可能导致过拟合（overfit）
![](regress4.png)
- 如何判断过拟合与否
![](regress5.png)
## 2.2.3 抑制过拟合
- 增加训练数据样本；如果可获取数据有限，可以通过数据增强的方式生成额外的数据，例如图像识别中常用反转，拉伸等方法生成新样本
- 在多轮训练迭代的过程中，持续评估验证集的指标，一旦验证集性能开始下降立刻停止训练
- 通过正则化技术，在损失函数中，增加正则化项，约束模型的权重，减少过拟合；分析特征和预估目标之间的相关性，剔除相关性低甚至不相关的特征，减少过拟合风险
# 2.3 项目演示实例
## 2.3.1 项目介绍
- 这是一个波斯顿房价的数据集，收集了1400多个房屋信息，每个房子记录了79个相关的字段以及对应的售价，目标是训练集的数据构造一个房价预估模型，并且通过房子的特征，预估测试集中的房价，部分字段如下
```
- SalePrice: the property's sale price in dollars. This is the target variable that you're trying to predict.
- MSSubClass: The building class
- MSZoning: The general zoning classification
- LotFrontage: Linear feet of street connected to property
- LotArea: Lot size in square feet
- Street: Type of road access
- Alley: Type of alley access
- LotShape: General shape of property
- LandContour: Flatness of the property
...
```
## 2.3.2 探索性数据分析（EDA）
```Python
import pandas as pd
import numpy as np

df_train = pd.read_csv('data/data77/train.csv')
df_train.shape # (1460, 81)

df_test = pd.read_csv('data/data77/test.csv')
df_test.shape # (1459, 80)

# 合并处理训练集和测试集，简化特征转化工作
df_combined = pd.concat([df_train, df_test], axis=0)
df_combined.shape

# 观察数据的均值标准差分位点
df_combined.describe(include='all')
```
![](regress6.png)
```Python
# 观察前5行数据
df_combined.head()
```
![](regress7.png)
```Python
# 缺失值情况统计
df_na = df_combined.isna().sum().reset_index(name="missing_values")

# 计算缺失百分比，df_combined.shape返回一个(行数, 列数)的二元组，shape[0]为总行数
df_na["percentage"] = round((df_na["missing_values"] / df_combined.shape[0]) * 100, 2)

# 按照缺失比例的数值降序排列
df_na.sort_values(by="percentage", ascending=False)[:35]
```
![](regress8.png)
## 2.3.3 数据预处理
```Python
# 去除ID列
df_v1 = df_combined.drop(columns=["Id"], axis=1)
df_v1.shape # (2919, 80), from 81 features to 80 features

# 缺失值处理 - 类别特征
# 根据 data_description.txt处理相关特征
# 两种处理方式：1、取众数(mode) 2、空值默认为“无”
df_v1["MSZoning"] = df_v1["MSZoning"].fillna(df_v1["MSZoning"].mode()[0])
df_v1["Alley"] = df_v1["Alley"].fillna("NoAlleyAccess")
df_v1["Utilities"] = df_v1["Utilities"].fillna(df_v1["Utilities"].mode()[0])
df_v1["Exterior1st"] = df_v1["Exterior1st"].fillna(df_v1["Exterior1st"].mode()[0])
df_v1["Exterior2nd"] = df_v1["Exterior2nd"].fillna(df_v1["Exterior2nd"].mode()[0])
df_v1["MasVnrType"] = df_v1["MasVnrType"].fillna("None")
df_v1["BsmtQual"] = df_v1["BsmtQual"].fillna("NoBasement")
df_v1["BsmtCond"] = df_v1["BsmtCond"].fillna("NoBasement")
df_v1["BsmtExposure"] = df_v1["BsmtExposure"].fillna("NoBasement")
df_v1["BsmtFinType1"] = df_v1["BsmtFinType1"].fillna("NoBasement")
df_v1["BsmtFinType2"] = df_v1["BsmtFinType2"].fillna("NoBasement")
df_v1["Electrical"] = df_v1["Electrical"].fillna(df_v1["Electrical"].mode()[0])
df_v1["KitchenQual"] = df_v1["KitchenQual"].fillna(df_v1["KitchenQual"].mode()[0])
df_v1["Functional"] = df_v1["Functional"].fillna(df_v1["Functional"].mode()[0])
df_v1["FireplaceQu"] = df_v1["FireplaceQu"].fillna("NoFireplace")
df_v1["GarageType"] = df_v1["GarageType"].fillna("NoGarage")
df_v1["GarageFinish"] = df_v1["GarageFinish"].fillna("NoGarage")
df_v1["GarageQual"] = df_v1["GarageQual"].fillna("NoGarage")
df_v1["GarageCond"] = df_v1["GarageCond"].fillna("NoGarage")
df_v1["PoolQC"] = df_v1["PoolQC"].fillna("NoPool")
df_v1["Fence"] = df_v1["Fence"].fillna("NoFence")
df_v1["MiscFeature"] = df_v1["MiscFeature"].fillna("None")
df_v1["SaleType"] = df_v1["SaleType"].fillna(df_v1["SaleType"].mode()[0])
```

```Python
# 缺失值处理 - 数值特征
# 取出int和float类型特征
df_numeric = df_v1.select_dtypes(include=["int64", "float64"])
# 排除预估目标
df_numeric_independent = df_numeric.drop(columns=["SalePrice"], axis=1)
# 预估目标
df_numeric_dependent = df_numeric["SalePrice"]
# 取出非数值类特征（即类别类特征）
df_others = df_v1.select_dtypes(exclude=["int64", "float64"])
```

```Python
# 使用随机森林填补数值类缺失值，需要训练随机森林，运行耗时较长，约3~4min
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestRegressor

imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=0), max_iter=10, random_state=0)
data_imputed = imputer.fit_transform(df_numeric_independent)
```

```Python
# 将data_imputed重新转换为dataframe
df_imputed_v1 = pd.DataFrame(data_imputed, columns=df_numeric_independent.columns)

# 将填补缺失值后的最初为int类型的列强制转换为int
df_integer_list = df_v1.select_dtypes(include=["int64"]).columns.tolist()
for col in df_integer_list:
    df_imputed_v1[col] = df_imputed_v1[col].astype("int64")

# 将填补缺失值后的df和类别类特征重新拼接起来
df_imputed_v2 = pd.concat([df_others.reset_index(drop=True), df_imputed_v1.reset_index(drop=True), df_numeric_dependent.reset_index(drop=True)], axis=1)
```

```Python
# 查看处理缺失值后的数据情况，除了SalePrice外其余字段均为非空
df_na2 = df_imputed_v2.isna().sum().reset_index(name="missing_values")
df_na2["missing percentage"] = round((df_na2["missing_values"] / df_imputed_v2.shape[0]) * 100, 2)
df_na2.sort_values(by="missing percentage", ascending=False)
```
![](regress9.png)
```Python
# 保存为 fillin_data.csv
df_imputed_v2.to_csv("fillin_data.csv", index=False)
```

```Python
# 特征转换，新增特征列
df_v2 = df_imputed_v2.copy()
df_v2.shape

# 定义：建成年限 = 销售年份 - 建造年份
df_v2["BuildingAge"] = df_v2["YrSold"] - df_v2["YearBuilt"]
df_v2["BuildingAge"] = df_v2["BuildingAge"].apply(lambda x: 0 if x < 0 else x)

# 定义：装修年限 = 销售年份 - 装修年份
df_v2["RemodelAge"] = df_v2["YrSold"] - df_v2["YearRemodAdd"]
df_v2["RemodelAge"] = df_v2["RemodelAge"].apply(lambda x: 0 if x < 0 else x)

# 定义：总质量 = 整体材料和表面处理质量 + 整体状况评级
df_v2["TotalQual"] = df_v2["OverallQual"] + df_v2["OverallCond"]

# 定义：总面积 = 地下室总面积 + 一楼面积 + 二楼面积
df_v2["TotalSF"] = df_v2["TotalBsmtSF"] + df_v2["1stFlrSF"] + df_v2["2ndFlrSF"]

# 是否有泳池：0-无 1-有
df_v2["Pool_Onehot"] = df_v2["PoolArea"].apply(lambda x: 1 if x > 0 else 0)

# 是否有壁炉：0-无 1-有
df_v2["Fireplace_Onehot"] = df_v2["Fireplaces"].apply(lambda x: 1 if x > 0 else 0)

# 是否有车库：0-无 1-有
df_v2["Garage_Onehot"] = df_v2["GarageArea"].apply(lambda x: 1 if x > 0 else 0)

# 删除无用特征
df_v3 = df_v2.drop(columns=["YearBuilt", "YearRemodAdd", "OverallQual", "OverallCond"], axis=1)
df_v3.shape # (2919, 83)
```
## 2.3.4 独热编码
```Python
# One-hot encoding，使用get_dummies函数快速进行独热编码
df_v4 = pd.get_dummies(df_v3)
df_v4.shape # (2919, 306)
```
## 2.3.5 标签转换
```Python
# 可视化
import matplotlib.pyplot as plt

# 绘制房价的频次直方图
plt.figure(figsize=(10, 6))
df_v4["SalePrice"].hist(bins=50)
plt.grid(True)
```
![](regress10.png)
```Python
# 房价分布接近帕累托分布，不满足部分回归模型对数据的假设（正态分布），考虑进行数据变换
# 由于帕累托分布为幂律分布，这里考虑取对数，并观察转换后的分布是否接近正态分布
df_v4["SalePrice"] = np.log1p(df_v4["SalePrice"])
plt.figure(figsize=(10, 6))
df_v4["SalePrice"].hist(bins=50)
plt.grid(True)
```
![](regress11.png)
## 2.3.6 模型预估
### 2.3.6.1 基本处理
```Python
train_dataset = df_v4[df_v4["SalePrice"].notnull()]
X = train_dataset.drop(columns=["SalePrice"], axis=1)
y = train_dataset["SalePrice"]
```

```Python
# 部分模型对输入数据范围敏感，做归一化（减均值、除以标准差）
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

```Python
# 从训练数据中按 0.8:0.2 的比例划分训练集，验证集
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2)
```

```Python
import math
# 评价指标相关，MSE越小越好，r2越大越好
from sklearn.metrics import mean_squared_error, r2_score
```

```Python
# 模型相关 
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
```
### 2.3.6.2 Ridge回归
```Python
model1 = Ridge(alpha=5) # alpha=5为模型超参数，可以针对不同任务进行调整，下同
model1.fit(X_train, y_train)
y_train_p = model1.predict(X_train)
print("Ridge train MSE:", mean_squared_error(y_train, y_train_p), " r2_score:", r2_score(y_train, y_train_p))
y_val_p = model1.predict(X_val)
print("Ridge val MSE:", mean_squared_error(y_val, y_val_p), " r2_score:", r2_score(y_val, y_val_p))

# Ridge train MSE: 0.00849589424880259  r2_score: 0.9471138463834101
# Ridge val MSE: 0.040316076976210806  r2_score: 0.7382497318982966
```
### 2.3.6.3 支持向量回归
```Python
model2 = SVR(C=10)
model2.fit(X_train, y_train)

y_train_p = model2.predict(X_train)

print("SVR train MSE:", mean_squared_error(y_train, y_train_p), " r2_score:", r2_score(y_train, y_train_p))

y_val_p = model2.predict(X_val)

print("SVR val MSE:", mean_squared_error(y_val, y_val_p), " r2_score:", r2_score(y_val, y_val_p))

# SVR train MSE: 0.005558720707366293  r2_score: 0.965397479225577
# SVR val MSE: 0.03557743594855112  r2_score: 0.7690151399552247
```
### 2.3.6.4 随机森林回归
```Python
model3 = RandomForestRegressor(n_estimators=20)
model3.fit(X_train, y_train)

y_train_p = model3.predict(X_train)

print("RandomForestRegressor train MSE:", mean_squared_error(y_train, y_train_p), " r2_score:", r2_score(y_train, y_train_p))

y_val_p = model3.predict(X_val)

print("RandomForestRegressor val MSE:", mean_squared_error(y_val, y_val_p), " r2_score:", r2_score(y_val, y_val_p))

# RandomForestRegressor train MSE: 0.003051204073674052  r2_score: 0.9810065376721664
# RandomForestRegressor val MSE: 0.01953739195556756  r2_score: 0.873154384902196
```
### 2.3.6.5 Xgboost
```Python
# Xgboost，运行耗时较长，约6~8min
model4 = XGBRegressor(objective='reg:squarederror', learning_rate=0.1, max_depth=3)
model4.fit(X_train, y_train)

y_train_p = model4.predict(X_train)

print("XGBRegressor train MSE:", mean_squared_error(y_train, y_train_p), " r2_score:", r2_score(y_train, y_train_p))

y_val_p = model4.predict(X_val)

print("XGBRegressor val MSE:", mean_squared_error(y_val, y_val_p), " r2_score:", r2_score(y_val, y_val_p))

# XGBRegressor train MSE: 0.006303275539943273  r2_score: 0.9607626944579594
# XGBRegressor val MSE: 0.016068571498145136  r2_score: 0.8956755415430754
```
### 2.3.6.6 对上述模型进行集成
```Python
# 手动自定义权重：可以考虑将效果较好的模型权重调高(并非越高越好)
# 也可考虑使用线性回归模型找出更合适的参数
def f(p1, p2, p3, p4):
    return p1 * 0.1 + p2 * 0.1 + p3 * 0.2 + p4 * 0.6

train_p1 = model1.predict(X_train)
train_p2 = model2.predict(X_train)
train_p3 = model3.predict(X_train)
train_p4 = model4.predict(X_train)
y_train_p = f(train_p1, train_p2, train_p3, train_p4)

val_p1 = model1.predict(X_val)
val_p2 = model2.predict(X_val)
val_p3 = model3.predict(X_val)
val_p4 = model4.predict(X_val)
y_val_p = f(val_p1, val_p2, val_p3, val_p4)

print("Ensemble train MSE:", mean_squared_error(y_train, y_train_p), " r2_score:", r2_score(y_train, y_train_p))
print("Ensemble val MSE:", mean_squared_error(y_val, y_val_p), " r2_score:", r2_score(y_val, y_val_p))

# Ensemble train MSE: 0.004501396726485288  r2_score: 0.9719792229288085
# Ensemble val MSE: 0.01561751940021583  r2_score: 0.898603976460751
```
## 2.3.7 输出结果
```Python
test_dataset = df_v4[df_v4["SalePrice"].isnull()]
X = test_dataset.drop(columns=["SalePrice"], axis=1)
# 使用相同的归一化逻辑
X_scaled = scaler.transform(X)

test_p1 = model1.predict(X_scaled)
test_p2 = model2.predict(X_scaled)
test_p3 = model3.predict(X_scaled)
test_p4 = model4.predict(X_scaled)
y_test = f(test_p1, test_p2, test_p3, test_p4)

# 训练过程对预估目标取了对数，这里需要重新转换回去
y_test_final = np.expm1(y_test)
```

```Python
result_df = pd.read_csv("data/data77/test.csv")
result_df["SalePrice"] = y_test_final
result_df = result_df[["Id", "SalePrice"]]
plt.figure(figsize=(10, 6))
result_df["SalePrice"].hist(bins=50)
```
![](regress12.png)
```Python
# 保存结果
result_df.to_csv("result.csv", index=False)
```